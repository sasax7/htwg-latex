
\chapter{Related Work}
\label{chap:state-of-the-art}

The identification of building energy anomalies requires an algorithmic framework that aligns with the structural and physical constraints established in Chapter~\ref{chap:fundamentals}. This chapter examines the current State of the Art (SOTA)---defined as the most advanced level of development in a technical field---to identify the most suitable detection methodology for the multivariate and contextual requirements of the research.

Existing research is evaluated through a systematic analysis of established benchmarks, which are standardized tests used to compare the performance of different algorithms. The subsequent sections study these benchmarks to determine which methodologies demonstrate quantifiable results in resolving the complexities of energy consumption signals.

\section{Evaluation Frameworks and Metric Reliability}
\label{sec:success-metrics}

The objective assessment of anomaly detection performance requires metrics that accurately reflect physical detection capabilities while remaining robust against technical noise. Recent meta-analyses identify a systemic ``illusion of progress'' in the field, where flawed evaluation measures consistently overestimate model efficacy.

\subsection{Conventional Standards and Point-Wise Limitations}
\label{subsec:f1-standard}
The Standard-F1 score, defined as the harmonic mean of precision and recall, remains the most prevalent metric in current \ac{TSAD} benchmarks. While it provides a scalar value for imbalanced datasets, its application to temporal sequences introduces several structural flaws:

\begin{description}
	\item[Assumption of Independence:] Traditional F1 calculations treat each timestamp as an \ac{i.i.d.} observation, ignoring the sequential nature of building data where anomalies typically persist over an extended range.

	\item[Sensitivity to Temporal Misalignment:] Minor misalignments in human-labeled ground truth lead to disproportionate penalties. A prediction offset by a single interval is penalized as both a false positive and a false negative, despite identifying the onset of a fault.

	\item[Lack of Sequence Awareness:] Point-wise metrics do not distinguish between successful detection of brief spikes and long-duration system failures.
\end{description}

\subsection{Point-Adjustment and the Illusion of Progress}
\label{subsec:illusion-point-adjustment}
To compensate for the difficulty of point-wise detection, many benchmarks utilize \ac{PA} (Point-Adjustment). Under this heuristic, an entire anomalous segment is counted as a true positive if the model identifies at least one individual point within it. Research by \textcite{liu2024elephant} identifies this as the primary cause of inflated performance reports. \ac{PA} is shown to favor ``noisy'' anomaly scores, allowing random predictions to achieve F1 scores near 1.0, thereby masking the failure of imprecise models.

\subsection{Comparison of Evaluation Scenarios}
\label{subsec:metric-comparison}
To quantify these reliability issues, \textcite{liu2024elephant} utilize synthetic prediction scenarios compared against a ground truth anomaly. As illustrated in Figure~\ref{fig:metric-reliability}, these range from perfect alignment (S8) to varying degrees of temporal lag (S1--S5) and completely random noise.

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{images/vus-pr-vs-others.jpg}
\caption{Reliability analysis of evaluation measures across different anomaly prediction scenarios. The red segment at the top represents the ground truth anomaly label, followed by various prediction signals (S1--S12 and random). The adjacent table indicates the resulting scores for threshold-independent and threshold-dependent metrics. Adapted from Liu and Paparrizos~\cite{liu2024elephant}.}
\label{fig:metric-reliability}
\end{figure}

The adjacent data in the benchmark study reveals the specific failure points of conventional scoring:

\begin{description}
	\item[PA-F1 Bias:] The PA-F1 measure assigns a perfect score of 1.00 to scenarios with significant lags (S2, S3, S4). Most critically, it yields a high score (0.73) for continuous random data, suggesting a successful detection where none exists.

	\item[Standard-F1 Rigidity:] While the Standard-F1 identifies random noise effectively (0.12), it fails to reward proximity. Predictions occurring slightly outside the boundary (S1, S5) receive a score of 0.00, providing no credit for identifying a deviation near the fault's boundary.
\end{description}

\subsection{Volume Under the Surface (VUS-PR) as a Robust Solution}
\label{subsec:vus-pr}
To resolve these contradictions, \ac{VUS-PR} (Volume Under the Surface--Precision Recall) is established as a more reliable measure. Unlike traditional metrics, it evaluates performance across a range of thresholds and incorporates a tolerance buffer around anomaly boundaries.

The benchmark results demonstrate that \ac{VUS-PR} maintains low values for random signals (0.09) similar to the Standard-F1, yet provides graduated rewards for proximity in scenarios like S2 and S3 where Standard-F1 remains static. By operating in a threshold-independent manner, \ac{VUS-PR} ensures that the reported performance reflects the model's actual capability to identify the contextual shifts characteristic of building energy consumption.


\chapter{Related Work}
\label{chap:state-of-the-art}

The identification of building energy anomalies requires an algorithmic framework that aligns with the structural and physical constraints established in Chapter~\ref{chap:fundamentals}. This chapter examines the current State of the Art (SOTA)---defined as the most advanced level of development in a technical field---to identify the most suitable detection methodology for the multivariate and contextual requirements of the research.

Existing research is evaluated through a systematic analysis of established benchmarks. The subsequent sections study these benchmarks to determine which methodologies demonstrate quantifiable results in resolving the complexities of energy consumption signals.

\section{Reliability and Benchmarking: The TSB-AD Framework}
\label{sec:systemic-benchmarking-challenges}

The selection of an appropriate detection methodology is constrained by systemic issues within the existing research landscape. \textcite{liu2024elephant} identify these issues as the ``elephant in the room,'' demonstrating that apparent progress in \ac{TSAD} is often an artifact of flawed evaluation practices rather than algorithmic superiority.

\subsection{Systemic Flaws and Metric Reliability}
\label{subsec:data-deficiencies}

Historical results are often compromised by three documented data-level flaws. First, \gls{mislabeling} leads to artificially high false-negative rates. Second, a prevalent \gls{run-to-failure-bias} rewards models that simply prioritize temporal position. Finally, \glspl{unrealistic-anomaly-ratio} fail to reflect the rarity of faults in physical systems.

The ``illusion of progress'' is further attributed to point-wise metrics like \ac{PA-F1}, which facilitates a significant overestimation of model performance by rewarding a detection if even a single point within an anomalous segment is identified. To ensure accuracy, this research adopts \ac{VUS-PR}, established by \textcite{liu2024elephant} as the robust standard for providing threshold-independent evaluation resistant to temporal lags and noisy scoring.

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{images/vus-pr-vs-others.jpg}
\caption{Reliability analysis of evaluation measures across different anomaly prediction scenarios. The red segment at the top represents the ground truth anomaly label, followed by various prediction signals (S1--S12 and random). The adjacent table indicates the resulting scores for threshold-independent and threshold-dependent metrics. Adapted from Liu and Paparrizos~\cite{liu2024elephant}.}
\label{fig:metric-reliability}
\end{figure}

\subsection{Benchmark Evaluation and Model Hierarchy}

Evaluation across 1\,070 curated time series reveals that statistical methods like \ac{Sub-PCA} dominate univariate settings, whereas deep learning architectures demonstrate superior modeling capacity in multivariate scenarios (\ac{TSB-AD}-M). As shown in Figure~\ref{fig:benchmark-ranking}, convolutional neural networks (\ac{CNN}) and generative models like \ac{OmniAnomaly} consistently outperform statistical baselines in capturing non-linear dependencies across multiple sensor channels.

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{images/benchmarkwinners.jpg}
\caption{Accuracy evaluation of the top 12 methods on (a) univariate (\ac{TSB-AD}-U) and (b) multivariate (\ac{TSB-AD}-M) datasets based on the \ac{VUS-PR} metric. Adapted from Liu and Paparrizos~\cite{liu2024elephant}.}
\label{fig:benchmark-ranking}
\end{figure}

\subsection{Implications for Multivariate Context Point Anomalies}
\label{sec:methodological-implications}

While the \ac{TSB-AD} benchmark provides a critical foundation for metric selection, its direct application to building energy telemetry is limited by several domain-specific gaps. The benchmark established that \ac{ML} architectures like \ac{CNN} excel in multivariate dependency modeling, while foundation models demonstrate superior efficacy in point anomaly identification. However, the \ac{TSB-AD}-M partition contains a limited representation of multivariate point anomalies; the majority of its instances consist of sequence-based deviations or global outliers rather than contextual point anomalies.

Furthermore, \textcite{liu2024elephant} primarily evaluated foundation models in univariate contexts, leaving their performance in multivariate environments unexplored. This is partly due to the historical limitation of these models to univariate inputs, a constraint only recently addressed by the emergence of architectures such as \textit{Chronos-2}, which extends universal forecasting to multivariate data~\parencite{ansari2025chronos}.

For building energy systems, the benchmark lacks specific energy-sector data and does not account for the longitudinal nature of building operations. In real-world scenarios, researchers often have access to multiple years of historical data, which allows for the establishment of robust baselines. Unlike the static snapshots in many benchmarks, building data is subject to slow behavioral drifts (e.g., equipment aging). This necessitates a benchmark setup where models can continuously learn from historical patterns before being evaluated on anomalies. Consequently, while this thesis utilizes \ac{VUS-PR} and the architectural hints provided by \textcite{liu2024elephant}, the experimental design is adapted to leverage long-term historical training and the specific requirements of multivariate contextual detection.

\section{Comparative Analysis of Deep Learning and Foundation Models in Energy Systems}

The landscape of \ac{TSAD} within energy data has shifted from classical statistical heuristics toward complex \ac{DL} architectures and, more recently, \ac{FM}. While \textcite{morshedi2025dl-iot-ad} provide a comprehensive overview of the efficacy of \ac{CNN}, \ac{LSTM}, and \ac{GAN} in \ac{IoT} environments, the specific requirements of building energy telemetry necessitate a more nuanced evaluation of model topology and predictive mechanisms.

\subsection{Deep Generative Models and the Advantage of Reconstruction}

A critical distinction in building energy research involves the choice between deterministic and generative modeling. \textcite{AZZALINI2025115069} conducted an empirical evaluation of deep autoencoders, demonstrating that \ac{LSTM}-based architectures generally outperform convolutional variants due to their ability to capture long-range temporal dependencies in sequential meter data.

A significant finding in this context is the superiority of \ac{RP} over simple \ac{RE}. In \ac{VAE} frameworks, \ac{RP} accounts for the variability of the reconstruction by utilizing the learned variance, which makes the model more robust against the inherent noise of building systems. This aligns with the architecture of \ac{OmniAnomaly}, a deep generative model that leverages stochastic recurrent neural networks to capture the normal patterns of multivariate data through robust latent representations~\parencite{su2019robust}.

\subsection{The Emergence of Time-Series Foundation Models}

The applicability of generalized models to the energy domain remains a primary research gap. \textcite{hela2025tsfm-energy-ad} investigated the performance of \ac{TSFM} such as \textit{TimeGPT} and \textit{MOMENT}, finding that while \ac{FM} demonstrate strong zero-shot capabilities for point anomalies, they often struggle with the domain-specific complexities of energy systems without proper adaptation.

However, the recent emergence of architectures capable of handling multivariate dependencies, such as \textit{Chronos-2}, suggests that the limitations observed in earlier benchmarks may be overcome~\parencite{ansari2025chronos}. These models utilize a ``universal forecasting'' approach that treats multivariate signals as tokens, allowing the model to learn cross-channel correlations that are essential for identifying context-based deviations in building energy consumption.

\subsection{Synthesis: Prediction-Based Stochastic Modeling}

A synthesis of current literature reveals a converging trend toward prediction-based methodologies as the primary solution for multivariate context point anomalies in building energy data (see Section~\ref{subsec:taxonomy-detection-methods}). While reported results remain susceptible to the data-level deficiencies and biased metrics identified in Section~\ref{subsec:data-deficiencies}, the scientific consensus points toward a transition from deterministic modeling to stochastically adjusted prediction. It was shown through the findings of \textcite{AZZALINI2025115069}, \textcite{su2019robust}, and \textcite{ansari2025chronos} that robust results are achieved when a model outputs a probability distribution rather than a single point value.

This development is consistent with the established physical nature of building telemetry. As described in Section~\ref{subsec:statistical-distribution-stochastic-noise}, energy signals are inherently stochastic, characterized by non-normal mixture distributions and multimodal profiles that originate from the interaction of discrete system states and irregular human behavior. Because traditional mean-based detection is ineffective for such data, the prediction model must inherently mimic these stochastic properties to establish a reliable normative baseline.

While advanced \ac{FM} and generative models define the current frontier of this stochastic approach, standard \ac{ML} architectures---specifically \ac{CNN} variants---maintain a vital role in the field due to their verified capacity to model non-linear spatial dependencies across multiple sensor channels. Conversely, while simple statistical methods remain valuable for validation, they lack the complexity required to resolve context-dependent deviations within the multi-year historical datasets utilized in this research. This project, therefore, prioritizes architectures that utilize stochastic prediction logic. Whether implemented via the multivariate forecasting of \textit{Chronos-2} or the deep generative reconstruction of \ac{OmniAnomaly}, the integration of probability distributions remains the verified standard for resolving the complexities of building energy telemetry.

\section{TODO: Mixture Density Networks for Stochastic Modeling}

This section will introduce Mixture Density Networks (MDN) as a mechanism for producing probabilistic outputs in anomaly detection. It will summarize the original MDN formulation, discuss how a \ac{CNN} feature extractor can be combined with an MDN output head to model multimodal distributions in building energy data, and position this approach relative to existing generative models such as \ac{VAE} and \ac{OmniAnomaly}.

\section{TODO: Root Cause Analysis for Anomaly Detection}

This section will review existing approaches to anomaly diagnosis and root cause analysis in time series systems. It will cover methods for attributing detected anomalies to specific sensors or subsystems, discuss graph- or topology-based RCA in building management, and examine how anomaly scores can be translated into operational and financial impact for facility managers.

\section{Identification of Research Gaps}

The current state of the art, summarized by the findings of \textcite{liu2024elephant}, reveals that existing benchmarks and models are insufficient for the specific requirements of building energy telemetry. The subsequent sections identify the primary gaps that inform the methodology developed in this research.

\subsection{Representation Gap in Multivariate Context Point Anomalies}

A primary deficiency in existing benchmark datasets is the lack of specific representation for Multivariate Context Point Anomalies (MCPA). Most established benchmarks focus predominantly on sequence anomalies or global outliers, leaving the interaction of multivariate context point deviations under-explored. Furthermore, existing archives often lack authentic energy consumption data, utilizing synthetic or unrelated \ac{IoT} datasets that do not exhibit the multimodal and stochastic characteristics inherent to building telemetry.

\subsection{Temporal Baseline and Generalization Gap}

The relationship between training data length and model performance in non-stationary building environments is currently not well-documented. Because building behavior evolves over time due to equipment degradation and seasonal shifts, it remains unclear how models trained on short-term baseline periods, such as two weeks, compare to those utilizing long-term historical data of one year or more. Additionally, there is a lack of empirical evidence regarding the seasonal translation capability of models. Specifically, it has not been sufficiently proven whether a model trained exclusively on winter heating cycles can accurately identify anomalies during summer cooling cycles without generating excessive false positives.

\subsection{Architectural Gap in Stochastic Mixture Modeling}

While \ac{TSFM} have shown quantifiable results in univariate forecasting, their application to multivariate energy anomaly detection represents a very recent development. The emergence of \textit{Chronos-2} provides a technical opportunity to evaluate universal forecasting on multivariate building data. However, a significant gap remains in combining the spatial feature extraction capacity of \ac{CNN} architectures with a stochastic output head. By integrating \ac{CNN} variants with Mixture Density Networks (MDN), a model could output a probability distribution instead of a deterministic point value, which would align more closely with the multimodal nature of building energy data.

\subsection{Diagnostic and Economic Functional Gap}

The majority of current research in \ac{TSAD} terminates at the detection phase, providing binary or continuous anomaly scores without further diagnostic depth. In building management, an alert without context is of limited operational utility. A critical gap exists in the automated identification of the root cause, which refers to the specific sensor or subsystem responsible for the deviation. Furthermore, there is a lack of integrated frameworks that translate detected energy waste into financial impact, defined as the monetary loss incurred by the anomaly over a specific time interval.

\section{Synthesis of Research Objectives}

The objective of this research is to fill these identified gaps by developing a custom benchmark tailored to building energy telemetry. By utilizing the \ac{VUS-PR} metric as the evaluative standard, the methodology focuses on the detection of multivariate context point anomalies across varying training baseline lengths. The proposed approach integrates the spatial modeling success of \ac{CNN} architectures with stochastic mixture density outputs and evaluates the multivariate capabilities of \textit{Chronos-2}. Ultimately, the framework aims to extend the detection pipeline into automated root cause analysis and financial impact quantification to provide actionable insights for building operations.
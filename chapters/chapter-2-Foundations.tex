\chapter{Foundations}
\label{chap:fundamentals}
This chapter establishes the theoretical and methodological foundations required for a comprehensive understanding of the subsequent research. The analysis begins with an examination of the technical characteristics and physical composition of building energy data to define the operational parameters. Subsequently, the fundamental concepts of anomalies are explored, encompassing the classification of specific types and their manifestation in temporal data. This positioning allows for the integration of the current use case into the broader field of \ac{TSAD}. Finally, Chapter~\ref{chap:state-of-the-art} traces the technical evolution of detection methodologies and provides a systematic review of the current \ac{SOTA}.
\section{Characteristics of Building Energy Data}
\label{sec:building-energy-data}
To establish the context for effective anomaly detection, the physical properties and behavioral characteristics of building energy data must first be examined. This section provides an analytical study of the data's composition and the underlying factors that determine its structure.

\subsection{Building Energy as Multivariate Time Series}
\label{subsec:building-energy-multivariate-ts}
Energy monitoring produces data in the form of a multivariate time series. To analyze these signals effectively, the mathematical structure of the data must first be defined.

\begin{description}
	\item[Time Series:] A time series is a sequence of data points recorded at successive, typically equal, time intervals. In building automation, these observations represent the continuous state of the system over time.

	\item[Multivariate Nature:] Unlike univariate data, which only tracks energy consumption, a multivariate time series captures multiple time-dependent variables simultaneously. In this research, the data includes not only the main meter readings but also influencing factors such as outdoor temperature, humidity, occupancy counts, and control setpoints.

	\item[Interdependence:] The variables in a multivariate building dataset are not independent. Changes in one variable (e.g., an increase in outdoor temperature) lead to changes in another (e.g., cooling power consumption). Effective anomaly detection must therefore account for these cross-variable dependencies rather than treating each series in isolation.
\end{description}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{images/multivariate-timeseries.png}
	\caption{Representative multivariate time series showing the main meter load together with occupancy (people count), outdoor temperature, solar radiation, humidity, and selected heating and cooling pumps over one week. The plot illustrates how multiple interdependent variables evolve jointly over time.}
	\label{fig:multivariate-timeseries}
\end{figure}

\subsection{The Causal Chain of Energy Consumption}
\label{subsec:causal-chain-energy-consumption}
The energy profile of a building is governed by a causal chain that describes the sequential relationship between demand, control, and consumption. Within this framework, every active device—from large-scale industrial machinery to individual lighting units—contributes to the aggregate electrical load.

The overall system operates through interdependent technical layers:

\begin{description}
	\item[Demand Layer:] Environmental factors or occupancy patterns create a requirement for a specific service (e.g., thermal comfort, lighting, or ventilation).
	\item[Control Layer:] Sensors and controllers interpret this demand and translate it into control signals that regulate the activity of technical systems.
	\item[Supply Layer:] Mechanical and electrical components activate to fulfill the requirement, resulting in measurable energy use.
\end{description}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{images/Energy-Causal-Chain.jpg}
	\caption{Causal chain of building energy consumption from demand over control to supply layer.}
	\label{fig:energy-causal-chain}
\end{figure}

Understanding the causal chain, as illustrated in Figure~\ref{fig:energy-causal-chain}, is a prerequisite for localizing anomalous behavior within building systems. A deviation observed in the building’s main meter often originates from a fault located in a preceding stage of the technical hierarchy, such as a sensor error or a logic failure in the control layer.

For instance, a malfunctioning temperature sensor reporting an erroneous heat spike triggers a cascade of responses. The control layer interprets this false data as a thermal requirement and initiates a cooling command to counteract the perceived heat. This signal causes the supply layer to activate mechanical components, such as cooling pumps and compressors. These devices consume electrical energy to satisfy the requested cooling load. Consequently, the aggregate output layer, represented by the building's main energy meter, records a significant increase in consumption. In this scenario, the measured energy spike is not a result of an actual physical need but acts as a symptom of a failure located deeper in the technical hierarchy.
\subsubsection{Environmental and Technical HVAC Drivers}
\label{subsec:hvac-environmental-drivers}
\ac{HVAC} systems represent the largest share of energy consumption in most commercial and residential buildings. The operational demand of these systems follows a direct causal sequence from external conditions to mechanical execution:
\begin{description}
    \item[Meteorological Inputs:] The thermal load is directly proportional to the gradient between the setpoint  and the outdoor temperature. Solar radiation and humidity further increase the cooling demand on compressors. For instance, high solar gains on a glass facade necessitate intensive cooling even if the outdoor temperature remains moderate.
    \item[System Logic and Operational Scheduling:] The efficiency of the energy transformation is determined by the control algorithms and predefined temporal patterns. Suboptimal setpoints, such as heating and cooling active simultaneously due to tight deadbands, result in excessive consumption. Scheduling determines the night setback periods; however, if a system heats an empty building at night because of a static schedule, a technically ``normal'' but inefficient energy load is generated.
    \item[Physical System Integrity:] The degradation of mechanical components causes a gradual increase in the power required for the same thermal output. Malfunctioning valves or clogged filters increase the static pressure in ventilation ducts, forcing fans to operate at higher speeds. This degradation results in a drift where the energy baseline slowly rises over time.
\end{description}

\subsubsection{Internal Loads and Occupancy Dynamics}
\label{subsec:internal-loads-occupancy}
The internal energy profile is shaped by the presence and behavior of building users, introducing a stochastic element to the data.
\begin{description}
    \item[Human-Driven Consumption and Thermal Gains:] Occupants influence the load directly via lighting and appliances. Additionally, human metabolism and the operation of hardware increase the CO$_2$ concentration and ambient temperature. This triggers higher ventilation rates and cooling demand.
    \item[Behavioral Interventions:] Manual actions by users can decouple the energy load from the expected environmental drivers. For example, opening a window during the heating season causes an immediate drop in local temperature, triggering the \ac{HVAC} system to ramp up. Conversely, shutting blinds reduces solar radiation, which lowers the cooling load on sunny days.
    \item[Integrated IT Infrastructure:] Buildings with server rooms exhibit a demand driven by computational load. High server activity increases power consumption and heat production, whereas a system outage results in an immediate drop to the base load (\ac{BL}).
\end{description}

\subsubsection{Structural and Technical Moderators}
\label{subsec:structural-technical-influences}
The physical environment acts as a moderator for the demand generated by the drivers mentioned above.
\begin{description}
    \item[Building Envelope and Thermal Inertia:] High-quality insulation reduces the energy required to compensate for external fluctuations. The thermal mass of the building prevents instantaneous temperature changes, creating a delay between an external heat spike and the resulting increase in cooling energy.
    \item[System Interdependencies:] Causal relationships exist between different building services. High-intensity lighting generates waste heat, which increases the load on the cooling system. Therefore, an \gls{anomaly} in the lighting schedule often manifests as a secondary anomaly in the cooling consumption.
	\item[Metering and Data Integrity:] The accuracy of the digital record depends on the stability of the measurement infrastructure. Technical noise, such as sensor drift or transmission errors, can create digital artifacts—errors that appear as anomalies in the data but have no physical cause. A common example is a communication rebound: if a meter goes offline and later reconnects, it may send all the ``missed'' energy data in a single massive spike. Data engineering must therefore distinguish between a physical surge in demand and these digital measurement errors to avoid false alarms.
\end{description}

\subsection{Temporal Autocorrelation and Persistence}
\label{subsec:temporal-autocorrelation-persistence}
The physical continuity of building systems leads to a high degree of Autocorrelation, where a measurement at a specific point in time is strongly dependent on its preceding values. This relationship is a direct consequence of the building's operational state and physical properties.

\begin{description}
	\item[Thermal Momentum:] The high thermal mass of building structures prevents instantaneous temperature changes. Consequently, the energy required for climate control is physically linked to the previous state of the system, creating a gradual rather than abrupt transition in demand.

	\item[Operational Inertia:] Technical systems, such as large ventilation fans or industrial boilers, require time to ramp up or shut down. This results in a continuous consumption curve where subsequent data points remain closely related.

	\item[State Persistence of High-Load Devices:] Large energy consumers, such as industrial chillers or production machinery, typically operate in sustained cycles. Once a device is activated, it remains in an ``on'' state for a significant duration to fulfill its operational purpose or to minimize mechanical wear from frequent switching. This creates sustained plateaus of energy demand in the time series.
\end{description}

This high degree of autocorrelation serves as both an advantage and a challenge for anomaly detection. Prediction-based models benefit from this structure because the high dependency on previous values makes the short-term behavior of the system highly predictable under normal conditions. However, this same persistence can hinder the detection of slowly developing faults or persistent higher energy consumption.

\subsection{Periodic Variations and Seasonality}
\label{subsec:periodic-variations-seasonality}
Building energy data is characterized by seasonality, which refers to regular and predictable fluctuations that recur over fixed intervals. These cycles are a direct consequence of the operational and environmental drivers described in the causal chain and are typically categorized into three temporal scales.

\begin{description}
	\item[Daily Trends:] The 24-hour day--night rhythm is the most dominant cycle, reflecting the primary patterns of solar radiation and standard occupancy. For instance, energy consumption typically ramps up at 07:00 as lighting and ventilation systems activate for arriving staff, and then tapers off in the evening during the night setback phase.

	\item[Weekly Trends:] These cycles distinguish between standard working days and weekends, resulting in distinct load profiles. A typical office building exhibits high consumption from Monday to Friday, followed by a significant drop on Saturday and Sunday when only the base load---such as emergency lighting and server cooling---remains active.

	\item[Seasonal Trends:] Annual weather changes shift the primary energy demand between heating and cooling over the course of a year. In a temperate climate, the ``heating season'' in winter creates a peak in gas or electrical heating demand, while the ``cooling season'' in summer causes high electricity consumption for air conditioning and chiller units.
\end{description}

The interaction of these trends creates a complex but repetitive fingerprint of a building's operation. Identifying an anomaly often requires recognizing when a measurement breaks one of these established patterns---for example, if a building shows weekday levels of energy consumption on a Sunday, it indicates a scheduling error in the control layer.

\subsection{Statistical Distribution and Stochastic Noise}
\label{subsec:statistical-distribution-stochastic-noise}
The statistical complexity of building energy data originates from the interaction of discrete system states, environmental extremes, and irregular human behavior. These factors create a distribution that deviates significantly from a standard normal model.

\begin{description}
	\item[Non-Normal and Mixture Distributions:] Energy data rarely follows a Gaussian distribution. Instead, it often manifests as a mixture distribution due to the discrete operational states of technical systems and the superposition of heterogeneous operating regimes. Even when examining a single main meter, the empirical histogram of 15-minute energy consumption typically exhibits multiple local maxima and a long tail, rather than a single symmetric bell curve. When multiple subsystems interact, the aggregate data forms a multimodal profile with several hills, making traditional mean-based detection ineffective.
\end{description}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{images/mixture-destribution.png}
	\caption{Empirical distribution of the building's main meter (15-minute kWh values, histogram) with an overlaid single normal distribution and a Gaussian mixture model with five components, illustrating the mismatch between a unimodal Gaussian model and the multimodal, heavy-tailed structure of real building energy data.}
	\label{fig:mixture-distribution}
\end{figure}

Figure~\ref{fig:mixture-distribution} illustrates this effect on real data: the measured main-meter consumption concentrates in several dense regions at lower loads and exhibits a pronounced right tail. A single normal distribution smooths over these structures and underestimates tail probabilities, whereas the fitted Gaussian mixture adapts to the multiple modes and better traces the empirical density.

\begin{description}
	\item[Causal Ambiguity and Stochastic Coincidence:] The high number of interdependent factors in the causal chain makes it difficult to distinguish between true causality and mere correlation. Since multiple stochastic events---such as irregular occupancy, specific weather patterns, and manual device activation---occur simultaneously, a ``perfect storm'' of unlikely but legitimate events can arise. If several mildly improbable things happen at once, the resulting energy spike may be an unlucky coincidence rather than a technical anomaly.

	\item[Sparse State Coverage of Weather and Operation:] A significant challenge is the incomplete coverage of the multivariate state space. Historical datasets often lack observations for extreme meteorological events, such as record-breaking low temperatures or unprecedented heatwaves. Similarly, specific operational corners---such as a full system restart or unique manual overrides---are seldom recorded. When these previously unvisited states are finally encountered, they exist outside the observed distribution and are frequently misidentified as anomalies, despite being physically valid.

	\item[Non-Stationarity:] Building energy data is frequently non-stationary, meaning its statistical properties, such as mean and variance, evolve over time. Equipment degradation, seasonal transitions, or lasting shifts in occupancy levels push the baseline. Consequently, a model trained on static data may flag normal operational states as anomalous because the underlying distribution has shifted.
\end{description}

\subsection{Data Acquisition and Semantic Structure}
\label{subsec:data-acquisition-telemetry}
The transition from physical energy consumption to digital analysis follows a multi-staged acquisition process that converts electrical quantities into a structured multivariate time series.

\begin{description}
	\item[Ontological Data Modeling:] To ensure the data is interpretable, the system applies a semantic schema or ontology~\cite{ElionaOntologies}. This framework does not merely label individual data points (e.g., ``Main Building Meter Consumption'') but also encodes the causal chain by defining the physical and logical relationships between meters and devices. By mapping how sub-meters and technical systems, such as \ac{HVAC} units, contribute to the aggregate load, the ontology provides the contextual intelligence necessary for detection models to localize faults within the technical hierarchy.
	\item[Standardized Units:] During the transfer from the physical meter to the software, raw readings are converted into standardized units, such as kWh, to ensure consistency across different hardware types.
\end{description}

\subsection{Data Continuity and Transmission Artifacts}
\label{subsec:communication-artifacts}
The reliability of the data stream is subject to the stability of the network infrastructure. Interruptions in this chain introduce non-physical distortions that must be distinguished from actual building faults.

\begin{description}
	\item[Transmission Gaps:] Connectivity issues between the building and the software create missing data points. These gaps break the temporal continuity of the record and require correction during the data cleaning stage.
	\item[Aggregation Spikes:] If a connection is restored after a period of downtime, the system may transmit the entire accumulated energy consumption from the offline period at once. Because the edge gateway recovers buffered data in this manner, the time series can exhibit a virtual spike. These peaks reflect a delay in data reporting rather than a physical surge in energy demand.
\end{description}

\subsection{Summary of Data Characteristics}
\label{subsec:synthesis-energy-data-complexity}
The examination of building energy data reveals a multi-layered structure defined by complex physical interdependencies and technical artifacts. These properties establish the operational environment for anomaly detection and determine the requirements for subsequent methodological selection. Because every electrical load is integrated into a causal chain driven by environmental and occupancy factors, effective detection cannot rely on univariate analysis. Instead, models must account for cross-correlations between the primary energy meter and exogenous drivers to resolve causal ambiguity. Furthermore, the presence of mixture distributions and heavy-tailed noise renders traditional mean-based or Gaussian detection methods ineffective. Algorithms must be capable of modeling multimodality and the extreme scarcity of anomalous observations within the multivariate state space.

Temporal characteristics, such as high autocorrelation and recurring seasonal cycles, necessitate models that interpret normality as a time-dependent and context-specific state rather than a static numerical range. Finally, the existence of transmission gaps and virtual spikes requires a clear distinction between physical system malfunctions and digital artifacts. Consequently, data integrity is not an inherent property of the telemetry stream but a condition that must be established through robust preprocessing. These characteristics provide the necessary context for the formal principles and taxonomies of anomaly detection.

\section{Foundations of Anomaly Detection}
\label{sec:fundamentals-anomaly-detection}
Anomaly detection is the process of identifying observations or patterns that deviate significantly from a defined notion of normality. In the specific context of time series data, an anomaly is defined not merely by its numerical value, but by its relationship to the temporal sequence. Unlike static data analysis, where outliers are detected in an independent and identically distributed feature space, \ac{TSAD} must account for the ordering, dependency, and trend-based characteristics of the signal.

The theoretical definitions and taxonomies established in this section follow the benchmark framework proposed by \textcite{paparrizos2022tsb}. Beyond the presentation of these fundamental concepts, this section systematically classifies the building energy data utilized in this research within the established taxonomies to define the specific requirements for the detection framework.

\subsection{Dimensionality and Modes of Normality}
\label{subsec:dimensionality-normality-modes}
\label{subsec:type-ts-normality-modes} The complexity of the detection task is governed by the structural type of the time series and the diversity of its underlying operational regimes. Based on the dimensionality and the number of normal states, the data is classified as follows:
\begin{description}
	\item[Dimensionality: Univariate vs. Multivariate:] A time series is univariate if it consists of a single time-dependent variable, such as the aggregate energy consumption of a building. In contrast, multivariate time series capture multiple interdependent variables simultaneously. The dataset utilized in this research is strictly multivariate, as it integrates energy consumption with exogenous drivers like outdoor temperature and occupancy. This multidimensional approach is necessary to resolve causal ambiguity; for example, a high cooling load is only interpretable when compared against a concurrent heatwave or high occupancy count.
	\item[Modality: Single-Mode vs. Multi-Mode Normality:] A system exhibits single-mode normality if its behavior follows a consistent, unified pattern. However, building energy systems typically operate in multi-mode normality regimes. This means that ``normal'' behavior shifts depending on the operational context. An intuitive example is the seasonal transition of an \ac{HVAC} system: a high electrical load during a summer afternoon is a normal response to cooling demand, whereas the same load profile in winter---where heating is primarily gas-driven---would be highly anomalous.
\end{description}

The data in this work is therefore characterized as a multivariate, multi-mode time series. This classification necessitates detection algorithms that can learn complex cross-variable correlations and adapt to shifting baselines without generating false positives during seasonal transitions.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{images/Univariate-Multivariate-normalities.jpg}
	\caption{Schematic illustration of time series types along two axes---dimensionality (univariate vs.~multivariate) and normality regimes (single-mode vs.~multi-mode). Adapted from Boniol et al.'s tutorial on new trends in time series anomaly detection~\cite{Boniol2023NewTrends}.}
	\label{fig:univariate-multivariate-normalities}
\end{figure}

\subsection{Taxonomy of Anomalies}
\label{subsec:taxonomy-anomalies}
Anomalies are classified based on their structural characteristics within the time series and their patterns of occurrence. This systematic categorization, as illustrated in Figure~\ref{fig:anomaly-taxonomy}, is essential for understanding the nature of deviations and selecting appropriate detection methodologies.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{images/types-anomalies.jpg}
	\caption{Taxonomy of time series anomalies along structural and multiplicity dimensions, distinguishing global and contextual point anomalies, subsequence-based anomalies, and their occurrence as single, multiple different, or multiple similar events. Adapted from Boniol et al.'s tutorial on new trends in time series anomaly detection~\cite{Boniol2023NewTrends}.}
	\label{fig:anomaly-taxonomy}
\end{figure}

\subsubsection{Structural Classification}
The structural classification differentiates between anomalies that appear as individual data points and those that manifest as sequences over time.

\begin{description}
	\item[Point-Based Anomalies:] Individual observations that deviate from the expected behavior. Multiple such anomalous points may occur consecutively; this alone does not constitute a subsequence-based anomaly but remains a series of point anomalies as long as each timestamp can be evaluated independently.
	\item[Global Point Anomalies:] A data point is considered a global anomaly if its value falls entirely outside the range of healthy values observed in the historical dataset (see Figure~\ref{fig:anomaly-taxonomy}~a.1.1). In building energy data, such extremes are often caused by sensor malfunctions or virtual spikes from communication rebounds.
	\item[Contextual Anomalies:] A data point is considered a contextual anomaly if it deviates from the expected behavior within a specific local context, even if its value lies within the global healthy range (see Figure~\ref{fig:anomaly-taxonomy}~a.1.2). The context is typically defined by temporal attributes (e.g., time of day, weekday vs.~weekend) or external covariates such as weather and occupancy.
	\item[Subsequence-Based Anomalies:] Anomalies that arise when a contiguous sequence of data points exhibits abnormal behavior, even if the individual points within that sequence are not necessarily anomalous on their own (see Figure~\ref{fig:anomaly-taxonomy}~a.2). These are often referred to as collective anomalies, where the pattern of the sequence itself is deviant~\cite{paparrizos2022tsb}.
\end{description}

\subsubsection{Multiplicity and Similarity}
Anomalies are further categorized based on their frequency and similarity within the dataset.

\begin{description}
	\item[Single Anomalies:] Isolated anomalous events that occur at a specific point in time or within a single interval (see Figure~\ref{fig:anomaly-taxonomy}~b.1).
	\item[Multiple Anomalies:] Situations in which several anomalous events occur within the dataset.
	\item[Multiple Different Anomalies:] The occurrence of various distinct anomaly types (see Figure~\ref{fig:anomaly-taxonomy}~b.2.1), such as a combination of sensor faults and control logic errors.
	\item[Multiple Similar Anomalies:] The recurrence of the same anomaly pattern over time (see Figure~\ref{fig:anomaly-taxonomy}~b.2.2), often indicative of a persistent fault or systematically inefficient operation.
\end{description}

\subsubsection{Classification within the Research Context}
The research presented in this work primarily focuses on the detection of contextual point anomalies and addresses the challenges posed by multiple similar anomalies.

\paragraph{Emphasis on Contextual Points.} The detection framework prioritizes contextual point anomalies to account for the influence of external drivers. For example, an extraordinarily cold day may result in energy consumption that reaches a historical maximum. While this would be flagged as a global point anomaly, it represents a normal response to the environmental context. Conversely, a light left on overnight might yield a value that appears low on a global scale but is significantly higher than expected for the nighttime context. True anomalies are therefore defined by their deviation from the expected load given the prevailing conditions.

\paragraph{Manifestation of Sequence Deviations.} High-frequency subsequence anomalies, such as an \ac{HVAC} unit switching on and off every minute, are also captured as contextual point anomalies in aggregated energy data. Although the fault originates as a rhythmic sequence, the cumulative effect of frequent starts produces an interval total (e.g., over 15 minutes or one hour) that is significantly higher than typical. Because substantial subsequence deviations manifest as detectable point outliers in the aggregated series, the methodological focus remains on the identification of contextual point anomalies.

\paragraph{Challenge of Recurring Patterns.} Systematic faults frequently produce multiple similar anomalies. A central challenge is the risk of normality drift: if an anomaly occurs with high regularity, the detection model may gradually incorporate the deviation into its learned definition of normal behavior. Without safeguards, this adaptation suppresses alerts despite the presence of a persistent inefficiency.

\section{Methodological Approaches to Anomaly Detection}
\label{sec:methodological-approaches}
The detection of anomalies transforms raw time series into actionable intelligence by quantifying deviation and selecting algorithms that match the available supervision signal.

\subsection{The Anomaly Score}
\label{subsec:anomaly-score}
Most anomaly detection methods are capable of generating an anomaly score for each individual data point in the series as an output. This output score is typically normalized to a range between $0$ and $1$, where higher values represent an increased probability of anomalous behavior.

To convert these continuous scores into actionable alerts, a numerical threshold is applied to the resulting sequence. If a threshold of 0.8 is selected, all data points with a score exceeding this limit are classified as anomalies, while values below the threshold are categorized as normal. As illustrated in Figure~\ref{fig:anomaly-score}, the anomaly score can be visualized alongside the underlying time series, with the threshold line separating nominal from anomalous observations.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{images/anomaly_score.png}
	\caption{Example of an anomaly score $s_i \in [0,1]$ aligned with the underlying time series. A threshold of 0.8 separates normal points from those flagged as anomalous.}
	\label{fig:anomaly-score}
\end{figure}

\subsection{Learning Paradigms in Energy Data}
\label{subsec:learning-paradigms}
The selection of an anomaly detection methodology is determined by the availability and nature of labeled training data. These strategies are categorized into supervised, semi-supervised, and unsupervised paradigms.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{images/superviced-unsuperviced.jpg}
	\caption{Schematic overview of supervised, semi-supervised, and unsupervised learning paradigms for anomaly detection, adapted from Boniol et al.'s tutorial on new trends in time series anomaly detection~\cite{Boniol2023NewTrends}.}
	\label{fig:learning-paradigms}
\end{figure}

\begin{description}
	\item[Supervised Learning:] Supervised approaches utilize training datasets that contain explicit labels for both normal operating states and specific anomaly examples (see Figure~\ref{fig:learning-paradigms}). In the domain of building operations, this paradigm is rarely applicable because facility managers and engineers seldom provide precise annotations of historical faults.
	\item[Semi-Supervised Learning:] Semi-supervised learning relies on a training dataset composed exclusively of healthy or normal examples (see Figure~\ref{fig:learning-paradigms}). This set serves as the baseline for the model to learn the characteristic patterns of a specific building. This approach is frequently employed in building energy management when historical data is agreed upon as a healthy reference or when the assumption is made that the majority of past operations were conducted without significant technical failure.
	\item[Unsupervised Learning:] Unsupervised learning operates without any prior labels or dedicated training phases based on healthy data (see Figure~\ref{fig:learning-paradigms}). The algorithm identifies anomalies by searching for statistically rare events or structural deviations within the current time series itself. This environment is typically encountered during the initial commissioning of a building when no historical record exists to establish a baseline.
\end{description}

\subsection{Taxonomy of Detection Methods}
\label{subsec:taxonomy-detection-methods}
Methodological approaches to anomaly detection are categorized into functional families based on their underlying logic. These families include distance-based, density-based, and prediction-based methods, as illustrated in the hierarchical taxonomy in Figure~\ref{fig:method-taxonomy}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{images/distance-dencity-forecast.jpg}
	\caption{Hierarchical taxonomy of anomaly detection methods grouped into distance-based, density-based, and prediction-based families, adapted from Boniol et al.'s tutorial on new trends in time series anomaly detection~\cite{Boniol2023NewTrends}.}
	\label{fig:method-taxonomy}
\end{figure}

\begin{description}
	\item[Distance-Based Methods:] These techniques identify anomalies by measuring the similarity between data sequences. Subsequences that exhibit a high distance from all other patterns in the dataset, often referred to as Discords, are classified as anomalous.
	\item[Density-Based Methods:] This approach evaluates the local density of data points within a feature space. Observations located in sparse regions, where the number of neighboring points is significantly lower than the average density, are identified as outliers.
	\item[Prediction-Based Methods:] These algorithms identify deviations by analyzing the difference between a model's generated representation of a signal and the actual observed measurement. This family encompasses Forecasting-based techniques, which typically utilize historical data to predict future values, and Reconstruction-based techniques, which learn to recreate the input data itself.
\end{description}

The research presented in this thesis focuses specifically on prediction-based methodologies. The technical justification for this prioritization, particularly regarding the integration of multivariate environmental drivers and the establishment of energy baselines, is provided in the subsequent chapters.

\section{Benchmarking Foundations}
\label{sec:benchmarking-foundations}

To determine how well an anomaly detection system works, it must be evaluated against a dataset where the correct outcomes are already known. This process is referred to as benchmarking and relies on comparing the model's decisions with a reference set of labels, the \gls{ground-truth}, to quantify detection performance.

\subsection{Binary Labeling and Ground Truth}
\label{subsec:binary-labeling-ground-truth}

The foundation of any benchmark is a labeled dataset in which each observation is assigned a binary label indicating whether it is considered normal or anomalous. In this context, a label of $0$ denotes normal operation and a label of $1$ denotes an anomaly, such as a fault or unusual event. When an algorithm analyzes this dataset, it produces its own sequence of binary decisions. Benchmarking assesses how closely these model-generated labels align with the original \gls{ground-truth} labels.

\subsection{The Confusion Matrix: Four Possible Outcomes}
\label{subsec:confusion-matrix}

When the model's predictions are compared to the \gls{ground-truth}, each observation falls into one of four categories. These outcomes are summarized in a \gls{confusion-matrix}, which serves as a scorecard for the detection system:

\begin{description}
	\item[True Positive (\ac{TP}):] The model correctly identifies an anomaly; the ground truth label is $1$ and the model predicts $1$.

	\item[True Negative (\ac{TN}):] The model correctly identifies normal operation; the ground truth label is $0$ and the model predicts $0$.

	\item[False Positive (\ac{FP}):] The model raises a false alarm; it predicts an anomaly ($1$) while the ground truth label is normal ($0$).

	\item[False Negative (\ac{FN}):] The model misses an anomaly; the ground truth label is $1$ but the model predicts normal operation ($0$).
\end{description}

These four counts form the quantitative basis for most evaluation metrics used in anomaly detection benchmarks.

\subsection{Measuring Success: Precision, Recall, and the F1 Score}
\label{subsec:precision-recall-f1}

The entries of the \gls{confusion-matrix} are used to derive summary metrics that characterize different aspects of a model's performance. Three central measures in anomaly detection are \gls{precision}, \gls{recall}, and the \ac{F1} score.

\gls{precision} quantifies how reliable the alarms are. It answers the question: when the model flags an anomaly, how often is this decision correct? Formally, precision is defined as the ratio of correctly detected anomalies to all observations that the model classified as anomalous,
\begin{equation}
	\mathrm{Precision} = \frac{TP}{TP + FP}.
\end{equation}
A high precision value indicates that the model rarely raises false alarms.

\gls{recall} quantifies how many anomalies are successfully detected. It answers the question: of all anomalies that actually occurred, how many did the model identify? Recall is defined as
\begin{equation}
	\mathrm{Recall} = \frac{TP}{TP + FN}.
\end{equation}
A high recall value indicates that the model is sensitive to anomalous behavior and misses few faults.

In practice, there is often a trade-off between \gls{precision} and \gls{recall}. A model that labels almost every point as anomalous may achieve high recall but very low precision, whereas an overly conservative model may exhibit the opposite behavior. The \ac{F1} score provides a single scalar summary by combining precision and recall through their harmonic mean,
\begin{equation}
	F1 = 2 \cdot \frac{\mathrm{Precision} \cdot \mathrm{Recall}}{\mathrm{Precision} + \mathrm{Recall}}.
\end{equation}

A high \ac{F1} score indicates that the model achieves a balanced compromise: it detects a large fraction of true anomalies while keeping the number of false alarms at a manageable level. In many benchmarking studies, this score is used as the primary indicator for ranking competing detection methods.

\section{Synthesis of Foundations}
\label{subsec:synthesis-foundations}
The examination of building energy data in Section~\ref{sec:building-energy-data} and the formal taxonomies of anomaly detection in Section~\ref{sec:fundamentals-anomaly-detection} reveal a highly specialized operational environment. Because building energy consumption is an aggregate signal driven by a complex causal chain (see Subsection~\ref{subsec:causal-chain-energy-consumption}), it is fundamentally characterized as a multivariate time series (see Subsection~\ref{subsec:building-energy-multivariate-ts}). This multidimensionality necessitates a focus on contextual point anomalies (see Subsection~\ref{subsec:taxonomy-anomalies}), as abnormality is defined primarily relative to the prevailing environmental and operational state rather than to a single global range. Furthermore, the temporal aggregation of energy data into 15-minute or hourly intervals (see Subsection~\ref{subsec:statistical-distribution-stochastic-noise}) effectively transforms high-frequency rhythmic faults into detectable point-based deviations in the aggregated series.

The implementation of detection strategies is constrained by several domain-specific factors. Human-driven consumption and manual behavioral interventions introduce a stochastic element to the time series (see Subsection~\ref{subsec:internal-loads-occupancy}), where these probabilistic variations deviate from standard Gaussian models and require detection frameworks capable of distinguishing between random noise and technical faults (see Subsection~\ref{subsec:statistical-distribution-stochastic-noise}). Simultaneously, the physical degradation of mechanical components causes a gradual increase in the power required for the same service output (see Subsection~\ref{subsec:structural-technical-influences}), ensuring that the definition of normality is non-stationary and undergoes baseline drift over long temporal scales.

Within this context, semi-supervised and unsupervised learning paradigms (see Subsection~\ref{subsec:learning-paradigms}) introduce a fundamental technical contradiction when historical data is utilized in the absence of expert-annotated fault histories. If persistent or repetitive anomalies were already present during the training period, the model incorrectly incorporates these deviations into its learned definition of normality. This risk of normality drift, particularly in the presence of multiple similar anomalies (see Subsection~\ref{subsec:taxonomy-anomalies}), leads to suppressed alerts and remains a central constraint for the subsequent methodological selection.

The establishment of these data-driven requirements and theoretical classifications lays the foundation for a systematic evaluation of the current state of the art in Chapter~\ref{chap:state-of-the-art}, where concrete algorithmic choices are assessed against the constraints of building energy data.

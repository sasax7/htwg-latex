\chapter{Fundamentals (Background)}
\label{chap:fundamentals}

% This chapter establishes the necessary theoretical background for the thesis.
% It defines the application domain, the data, the problem, and the
% fundamental concepts required to understand the proposed solution.

\section{Application Domain: Smart Buildings and Energy Management}
\label{sec:smart_buildings}

The building sector is currently facing a significant transformation, driven by two primary forces: 
rising public and corporate demand for sustainability (Environmental, Social, and Governance - ESG) 
and increasingly stringent government legislation aimed at energy monitoring and consumption reduction.

The urgency for this transformation is clear: 
buildings are responsible for approximately \textbf{40\% of global energy consumption} and a similar fraction of 
energy-related CO2 emissions \cite{TODO_Cite_IEA_or_EU_Commission}. This massive energy footprint represents both a critical 
challenge and a significant opportunity. Reducing this consumption is not only a legislative necessity but also a powerful 
lever for companies to meet ESG goals and reduce operational costs.

This trend is enabled by rapid advancements in the Internet of Things (IoT). 
IoT sensors and edge computing devices are becoming simultaneously more powerful and more cost-effective. 
This allows modern "Smart Buildings" to move beyond simple automation (like lighting or heating) towards comprehensive, 
high-frequency data collection. This creates a critical need to monitor all building subsystems, especially energy-related assets 
like HVAC (Heating, Ventilation, and Air Conditioning), lighting, and plug loads.

The data generated by these systems is finally received and stored in a centralized IoT platform or an \textbf{Energy Management Information System (EMIS)}.
In systems such as these, the raw time series data is processed, stored in time-series databases, and made available for 
visualization, alarming, and advanced analytics—which is the starting point for the work in this thesis.


\subsection{Data Sources and Acquisition in Smart Buildings}
\label{sec:data_sources}

The data in a smart building originates from a diverse set of heterogeneous sources. These systems must bridge the gap between traditional Operational Technology (OT) and modern Information Technology (IT).

\begin{itemize}
    \item \textbf{Building Management Systems (BMS):} This is the central "brain" of the building, often based on established 
    frameworks like \textbf{Tridium Niagara} or \textbf{openBOS}. It controls and monitors core assets like HVAC 
    (chillers, pumps, air handling units) and lighting. These systems traditionally communicate over building-specific 
    protocols like \textbf{BACnet} or \textbf{Modbus}.
    
    \item \textbf{Smart Meters (AMI):} Historically, utility consumption (electricity, gas, water) was tracked 
    by \textbf{legacy meters} that required manual, infrequent readings (e.g., annually). A key enabler for smart buildings is 
    the retrofitting or replacement of these with smart meters, which are part of an Advanced Metering Infrastructure (AMI). 
    While these meters *can* provide data at high frequencies (e.g., every second), a \textbf{15-minute interval} has become a 
    de-facto standard for energy balancing and building analytics.
    
    \item \textbf{IoT Sensors:} A third, rapidly growing source is a wide array of granular IoT sensors. 
    These are often wireless and provide high-resolution data on factors that influence energy consumption, 
    such as \textbf{CO2} levels, room \textbf{occupancy} (PIR sensors), \textbf{temperature}, and \textbf{humidity}.
\end{itemize}

% TODO: Diesen Teil vielleicht in einen eigenen Absatz "Data Pipeline" verschieben.
Data from these different sources is collected and unified by on-site \textbf{edge gateways} 
(which could be the BMS itself, like a Niagara JACE, or a separate device). 
These gateways poll the Modbus/BACnet devices, read the smart meters, and subscribe to IoT sensor data. 
The edge device then aggregates, standardizes, and transmits this data to the central EMIS, typically using lightweight, 
IT-friendly protocols like \textbf{MQTT}.


\section{Data Foundation: Time Series}
\label{sec:time_series}

Formally, a \textbf{time series} is a sequence of data points indexed in time order. 
In the context of this thesis, every time a sensor or meter in the building is read (e.g., a 15-minute energy value), 
a data point is generated. This data point consists of two components: the \textbf{timestamp} 
(the exact time the measurement was taken) and the \textbf{value} (the measurement itself). 

This sequential, time-ordered nature is the defining characteristic of time series data. 
It implies that data points are not independent; 
rather, each point has a temporal relationship with its predecessors and successors.
This structure is fundamental to the analysis, as patterns (like trends and seasonality) and anomalies are only revealed 
by observing the data in its correct sequence.

\subsection{Characteristics of Energy Time Series}
\label{sec:time_series_characteristics}
% TODO: Discuss specific characteristics of *energy* data (from your old template)
Energy time series data exhibits several key characteristics:
\begin{itemize}
    \item \textbf{Seasonality:} Daily (e.g., building operation hours), weekly (weekends vs. weekdays), and annual (summer vs. winter) cycles.
    \item \textbf{Trend:} (e.g., increasing consumption due to expansion, or decreasing due to efficiency measures).
    \item \textbf{Autocorrelation:} The value at time $t$ is highly dependent on the value at time $t-1$.
    \item \textbf{Noise:} Inherent randomness and measurement errors.
\end{itemize}

\subsection{Common Data Faults and Challenges}
\label{sec:data_faults}
While time series analysis is powerful, data from real-world building systems is notoriously "dirty" and presents significant 
challenges. The detection of these data faults is a prerequisite for any meaningful anomaly detection, as one must first 
distinguish between a *data error* and a true *process anomaly*.

A primary source of data faults comes from how consumption is calculated. Many utility meters (e.g., for electricity or gas) 
provide a \textbf{cumulative meter reading}, which is a monotonically increasing value. 
The interval consumption is then calculated by the edge gateway or platform as the difference between the current reading 
and the previous one.

This calculation is highly vulnerable to \textbf{connection errors}. If the edge gateway loses connection to the meter for a 
period (e.g., one hour), it cannot record the intermediate values. When the connection is re-established, it reads the new, 
higher meter value. The system then calculates the *entire* consumption that occurred during the downtime and 
logs it as a single data point. This creates a large, artificial \textbf{data spike} in the consumption time series, 
which does not represent a real surge in energy use but rather a data-logging artifact.

Other common data problems in this domain include:
\begin{itemize}
    \item \textbf{Missing Values:} Gaps in the data (often shown as `null` or `NaN`) due to connection loss, device power failure, 
    or database errors.
    \item \textbf{Stuck Values:} The sensor or meter "freezes" and reports the exact same value repeatedly over a long period, 
    which is physically improbable for dynamic values like power consumption or temperature.
    \item \textbf{Sensor Drift:} A gradual, systematic error where a sensor's accuracy degrades over time 
    (e.g., a CO2 sensor requires recalibration).
    \item \textbf{Out-of-Bounds Values:} Data points that are physically impossible, such as negative energy consumption 
    (for a non-producing asset) or an indoor temperature of 1000°C, which are clearly sensor failures.
\end{itemize}

\subsection{Common Data Preprocessing Techniques}
\label{sec:preprocessing}
Before a reliable anomaly detection model can be trained, the data faults described in Section \ref{sec:data_faults}
must be addressed. Raw building data is almost never suitable for direct analysis. 
The goal of preprocessing is to clean, standardize, and transform the data into a high-quality format suitable for machine learning
models.

Key preprocessing steps include:

\begin{itemize}
    \item \textbf{Handling Missing Values (Imputation):} Gaps in data (e.g., `NaN` values from connection loss) must be filled.
    \begin{itemize}
        \item \textbf{Simple Methods:} Techniques like \textbf{Linear Interpolation} 
        (drawing a straight line between two known points) or \textbf{Forward Fill} 
        (carrying the last known value forward) are fast but often inaccurate.
        \item \textbf{Statistical Methods:} More advanced models, like ARIMA, can be used to forecast and fill in small gaps 
        based on statistical properties.
        \item \textbf{Deep Learning \& Foundation Models:} For complex, seasonal data, modern approaches use deep learning models 
        (like LSTMs or Transformers) trained on similar, complete data to predict the missing values. 
        Recent \textbf{time series foundation models} (FMs) are particularly powerful, as they can be used for "zero-shot" 
        imputation—filling gaps by leveraging patterns learned from vast, unrelated datasets \cite{TODO_Cite_FM_Imputation_Paper}.
    \end{itemize}
    
    \item \textbf{Correcting Faulty Data:}
    \begin{itemize}
        \item \textbf{Cumulative Meter Spikes:} The most common fault—a large spike from a re-connection—is typically handled 
        by a specific algorithm. This involves detecting the spike, replacing it and the preceding gap with `NaN`, and then 
        using an imputation method (like linear interpolation or back-distribution) to evenly spread the total consumption 
        across the gap period.
        \item \textbf{Stuck Values:} These are often detected by finding runs of identical values (e.g., `value(t) == value(t-1)` 
        for N consecutive steps) and replacing them with `NaN` to be imputed.
        \item \textbf{Out-of-Bounds Values:} Simple clipping (Winsorizing) or removal of physically impossible data 
        (e.g., negative consumption, 1000°C) is a necessary first step.
    \end{itemize}

    \item \textbf{Resampling:} Since data can arrive at inconsistent intervals (e.g., 5 min, 15 min, 1h), all time series must 
    be resampled to a uniform frequency, such as the \textbf{15-minute} interval common in energy management. 
    This is typically done using an aggregation (e.g., `mean()`, `sum()`) or interpolation.

\subsection{Univariate vs. Multivariate Time Series}
\label{sec:multivariate_timeseries}

Time series data can be classified based on the number of variables recorded at each timestamp.

A \textbf{univariate} time series measures only a single variable over time. An example would be a dataset 
tracking *only* the total electricity consumption of a building. 
While simple, this approach ignores all external factors.

In contrast, a \textbf{multivariate} time series measures multiple variables simultaneously. 
In this case, the dataset would contain parallel, time-aligned sequences, such as electricity consumption, 
outside air temperature, and building occupancy.

Energy data is, by its very nature, a prime example of a multivariate problem. 
The energy consumption of a building is not an independent variable; it is heavily influenced by a multitude of other 
factors (\textbf{covariates}), such as weather, occupancy, and operational schedules. 
By capturing these rich dependencies, a multivariate model can learn a far more accurate and robust definition 
of "normal" behavior.

This thesis focuses on multivariate data specifically because this relationship between variables is the key to finding 
complex \textbf{contextual anomalies}—for example, identifying high energy use that would be normal on a cold day but is 
highly anomalous on a warm day.

\section{Problem Definition: Anomaly Detection}
\label{sec:anomaly_definition}

This section defines what an anomaly is and applies this definition to the specific context of building energy data.

Globally, an anomaly is a data point, or a sequence of points, that deviates significantly from the majority of the data or a 
predefined notion of "normal" behavior \cite{TODO_Chandola2009}. 
In the context of this thesis, this definition requires further refinement.
But let's first explore the different types of anomalies.

\subsection{Types of Anomalies}
\label{sec:anomaly_types}
% TODO: Define "anomaly" (also known as outlier, novelty, or deviant)
An anomaly is a data point, or a sequence of points, that deviates significantly from the majority of the data or a 
predefined notion of "normal" behavior \cite{TODO_Chandola2009}. 
Following the taxonomy from \cite{TODO_Paparrizos2025}, we distinguish:

\begin{description}
    \item[Point Anomalies:] A single data instance that is anomalous with respect to the rest of the data. 
    In energy data, this could be a true consumption spike (e.g., a large, unscheduled device activation) 
    or a data fault (e.g., a sensor failure).
    
    \item[Contextual Anomalies:] An instance that is anomalous only in a specific context. 
    This is the most critical anomaly type for energy management 
    (e.g., high heating energy consumption in summer is anomalous, but normal in winter). 
    This highlights why multivariate data (providing context like weather) is essential.
    
    \item[Collective (Sequence) Anomalies:] A collection of related data instances that are anomalous as a group, 
    even if individual points are not (e.g., a subtle but persistent unusual vibration pattern in an HVAC pump).
\end{description}

A critical distinction must be made between anomalies in the \textit{data logging process} 
(data faults, as discussed in Section \ref{sec:data_faults}) and anomalies in the \textit{physical energy consumption}.
For example, a spike caused by a temporary connection issue to a cumulative meter is a data anomaly, 
but not a real process anomaly. This thesis seeks to identify anomalies in the *real energy consumption*, 
not data pipeline errors.

Furthermore, not all statistical deviations are true anomalies. 
A high consumption spike every Monday morning caused by a scheduled AI-workstation training session is not an anomaly; 
it is a recurring, explainable pattern. A robust detection model must learn this context and correctly 
identify only \textit{unexpected} deviations.

Therefore, this thesis will primarily focus on detecting \textbf{Point} and \textbf{Contextual Anomalies} that signify real, 
unexplained deviations in energy usage. \textbf{Collective (Sequence) Anomalies} are not a primary focus. 
While they might occur (e.g., an HVAC unit cycling more rapidly than usual), the main consequence of such inefficiency is 
typically a higher-than-expected energy consumption, which will be detected as a point or contextual anomaly anyway. 
This marks a key difference from other domains like medical ECG analysis, where the precise *shape* of a sequence is paramount.


\section{Anomaly Scoring}
\label{sec:anomaly_scoring}

Most anomaly detection algorithms do not output a binary "normal" or "anomalous" label. Instead, 
they assign a continuous numerical value to each data point, known as an \textbf{anomaly score}. 
This score quantifies how "unusual" a data point is, given the learned model of normality.

By convention, this score is often normalized to a range between \textbf{0 and 1}.
\begin{itemize}
    \item A score of \textbf{0} (or close to 0) indicates that the data point is perfectly normal.
    \item A score of \textbf{1} (or close to 1) indicates that the data point is highly anomalous.
\end{itemize}

This approach provides more flexibility than a simple binary classification. 
To generate a concrete alarm, a human operator or a downstream process must set 
a \textbf{threshold} (e.g., 0.95). Any data point with a score exceeding this threshold is then flagged as an anomaly. 
The following methodologies describe different ways to generate this anomaly score.


\section{Taxonomy of Detection Methodologies}
\label{sec:method_taxonomy}

Detection methods can be broadly grouped into three main families, primarily differing in how they model "normal" data and, 
consequently, how they calculate the anomaly score \cite{TODO_Paparrizos2025}:

\begin{description}
    \item[Distance-based:] Assumes that normal data points are "close" to their neighbors. 
    The anomaly score is a function of the distance of a point to its nearest neighbors (e.g., k-NN).
    
    \item[Density-based:] Assumes that normal data points are located in dense regions. The anomaly score is derived from the 
    local data density (e.g., Isolation Forest calculates a score based on how easily a point is "isolated").
    
    \item[Prediction-based:] Assumes that normal data is predictable. A model learns the temporal patterns of the data. 
    The anomaly score is then based on the deviation between the model's expectation and the actual observed data.
\end{description}


\subsection{Focus: Prediction-Based Methodologies}
\label{sec:prediction_methods}

Given that energy data is a multivariate time series where context is critical, prediction-based methods are a natural fit. 
This family itself can be split into two main approaches based on how the model "predicts" the data:

\begin{itemize}
    \item \textbf{Reconstruction-based:} The model (e.g., an Autoencoder) is trained to compress and then reconstruct its own 
    input. It learns a compact representation of "normal" data. The anomaly score is 
    the \textbf{reconstruction error}—the difference between the original data point and its reconstructed version. 
    The assumption is that the model will be very good at reconstructing normal patterns but will fail to reconstruct 
    anomalous ones, resulting in a high error.
    
    \item \textbf{Forecasting-based:} The model is trained to predict the next time step(s) based on past data.
     The anomaly score is the \textbf{prediction error} (or \textbf{residual})—the difference between the model's forecast and 
     the actual value that occurs.
\end{itemize}

A more advanced version of the forecasting-based approach, and the focus of this thesis, is \textbf{Probabilistic Forecasting}. 
This method provides a key advantage over simple "point" predictions:
\begin{itemize}
    \item \textbf{Point Forecasting:} The model predicts a single value (e.g., 21.5°C). The anomaly score is the residual 
    (e.g., $| \text{actual} - \text{predicted} |$). This can be noisy if the model is uncertain but predicts a single "wrong" 
    value.
    
    \item \textbf{Probabilistic Forecasting:} The model predicts a full probability distribution, often represented by a 
    prediction interval (e.g., "The value will be between 21.0°C and 22.0°C with 95\% confidence").
\end{itemize}

This thesis will leverage probabilistic forecasting, defining an anomaly as a real observation that falls *outside* of the 
predicted confidence interval. This is a more robust approach as it accounts for the model's own uncertainty.

\section{Fundamentals of Evaluation}
\label{sec:evaluation_fundamentals}

To evaluate the performance of an anomaly detection model, we require labeled data (ground truth) and robust evaluation metrics. 
While large, public benchmark datasets for anomaly detection exist, such as TSB-UAD \cite{paparrizos2022tsb} and the more recent 
TSB-AD \cite{liu2024elephant}, only a small fraction of these datasets consist of multivariate energy data that exhibits complex 
contextual anomalies and the inherent chaotic nature of building operations. Therefore, to validate the methods in this thesis, 
a domain-specific, labeled dataset of multivariate energy time series is essential.

Given a labeled dataset and an anomaly score (as defined in Section \ref{sec:anomaly_scoring}), a binary decision is made by 
applying a \textbf{threshold}. Any score above this threshold is classified as "anomalous," and any score below it as "normal." 
This classification allows for the creation of a \textbf{Confusion Matrix}, which forms the basis of most standard metrics.

\subsection{Point-based Metrics}
\label{sec:point_metrics}

These metrics evaluate each time stamp independently, treating the entire time series as a collection of independent 
classification problems. The Confusion Matrix consists of:
\begin{itemize}
    \item \textbf{True Positives (TP):} An anomaly occurred, and the model correctly flagged it.
    \item \textbf{True Negatives (TN):} The data was normal, and the model correctly ignored it.
    \item \textbf{False Positives (FP):} The data was normal, but the model incorrectly flagged an anomaly (Type I Error).
    \item \textbf{False Negatives (FN):} An anomaly occurred, but the model failed to detect it (Type II Error).
\end{itemize}

From this matrix, metrics like \textbf{Precision} ($\frac{TP}{TP+FP}$) and \textbf{Recall} ($\frac{TP}{TP+FN}$) are derived. 
[cite_start]The \textbf{F1-Score} ($2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$) 
is the harmonic mean of Precision and Recall, providing a single score that balances the trade-off between FPs and 
FNs \cite[cite: 2557]{Tutorial_Advances in Time-Series Anomaly Detection.pdf}.

Since choosing a single threshold is problematic, threshold-independent metrics are often preferred. These metrics evaluate the 
model's performance across *all* possible thresholds:
\begin{itemize}
    [cite_start]\item \textbf{AUC-ROC (Area Under the Receiver Operating Characteristic Curve):} Plots the True Positive Rate 
    (Recall) against the False Positive Rate (FPR) \cite[cite: 2581]{Tutorial_Advances in Time-Series Anomaly Detection.pdf}.
    \item \textbf{AUC-PR (Area Under the Precision-Recall Curve):} Plots Precision against Recall. 
    [cite_start]This is often considered more informative than AUC-ROC for highly imbalanced datasets, 
    which is typical for anomaly detection~\cite{paparrizos2022tsb}.
\end{itemize}


\subsection{Sequence-based Metrics (VUS)}
\label{sec:sequence_metrics}

[cite_start]Standard point-based metrics like F1 and AUC-ROC have a critical flaw when applied to time series: 
they are highly sensitive to \textbf{misalignment} or \textbf{lag} \cite[cite: 2587-2602]{Tutorial_Advances in Time-Series Anomaly Detection.pdf}.

An anomaly detection model might correctly identify an anomalous event but flag it a few time steps early or late. 
[cite_start]A point-based metric would severely penalize this, double-counting the error: it would register 
a \textbf{False Positive} (for the misaligned detection) and a \textbf{False Negative} 
(for the "missed" ground-truth point) \cite[cite: 2607]{Tutorial_Advances in Time-Series Anomaly Detection.pdf}.

To address this, \textbf{Range-based} or \textbf{Event-based} metrics were developed. 
[cite_start]These metrics evaluate entire anomalous *events* and are more tolerant of minor time shifts, 
for example by accepting any detection within a "buffer" region around the true event \cite[cite: 2643]{Tutorial_Advances in Time-Series Anomaly Detection.pdf}.

A recent and highly robust metric in this category is the \textbf{VUS (Volume Under the Surface)}
 \cite{paparrizos2022volume, boniol2025vus}. 
 The core idea of VUS is to extend the concept of range-based evaluation. Instead of choosing one fixed buffer size, 
 VUS computes the AUC (e.g., AUC-PR) for *all possible buffer lengths*, from zero up to a predefined maximum. 
 [cite_start]This calculation creates a 3D surface (e.g., Precision, Recall, and Buffer Length), and the 
 "Volume Under the Surface" is integrated to provide a single, comprehensive 
 score \cite[cite: 2701, 2706]{Tutorial_Advances in Time-Series Anomaly Detection.pdf}.

[cite_start]VUS has been demonstrated to be significantly more robust to lag, noise, and other real-world data issues 
compared to traditional metrics \cite[cite: 2682]{Tutorial_Advances in Time-Series Anomaly Detection.pdf}. 
[cite_start]Given its suitability for event-based detection and its status as the most reliable evaluation measure \cite[cite: 3473, 3474]{TSB-AD-NeurIPS24 (1).pdf}, 
the \textbf{VUS-PR} (Volume Under the Precision-Recall Surface) is selected as the primary evaluation metric for this thesis.


\section{Learning Paradigms for Anomaly Detection}
\label{sec:learning_paradigms}

The methodologies introduced in Section \ref{sec:method_taxonomy} can be further categorized based on the type of data required 
for training. This "learning paradigm" dictates how a model learns "normal" behavior and is a critical choice in any 
real-world application.

\begin{description}
    \item[Supervised:] This approach requires a fully labeled dataset containing examples of *both* normal operation and various 
    types of anomalies. The model is trained as a standard classifier (e.g., "normal" vs. "fault type A"). 
    This is common in domains like medical diagnostics, where known anomalous patterns (e.g., specific arrhythmia in an ECG) 
    are well-documented and labeled by experts. However, this is impractical for building energy data, as it is impossible 
    to pre-collect and label all potential future faults.
    
    \item[Unsupervised:] The model receives a single dataset with no labels, operating under the assumption that anomalies are 
    "rare and different" from the majority of the data. The model must find these deviations without any prior reference 
    for what is normal or anomalous. Most distance-based and density-based methods are inherently unsupervised.
    
    \item[Semi-supervised:] This is the most practical and common approach for time-series anomaly detection. 
    The model is trained *only* on a "healthy" dataset that is assumed to represent a normal, fault-free baseline. 
    The model's sole objective is to learn a comprehensive representation of this normality. 
    When presented with new, unseen data, any point that deviates significantly from this learned baseline is flagged as an 
    anomaly.
\end{description}

In the domain of building energy management, we have a distinct advantage that aligns perfectly with 
the \textbf{semi-supervised paradigm}. Buildings often have months or even years of historical consumption data. 
While this data is unlabeled, it can be "curated"—for instance, by selecting a recent, representative period of operation—and 
be *assumed* to be a "mostly normal" baseline.

Therefore, this thesis will focus on \textbf{semi-supervised anomaly detection}. This approach allows us to leverage vast 
historical data to train robust models. However, it also introduces two specific challenges that this work will address:
\begin{itemize}
    \item \textbf{Baseline Contamination:} What if the historical "baseline" data already contains anomalies? The model might 
    incorrectly learn these faults as "normal" behavior.
    \item \textbf{Concept Drift:} How do we detect anomalies when the baseline itself changes 
    (e.g., due to equipment retrofits, new tenants, or seasonal changes) and the original "healthy" data is no longer 
    representative?
\end{itemize}

The following model architectures are particularly well-suited for this semi-supervised, prediction-based approach:

\begin{itemize}
    \item \textbf{Isolation Forest (IForest):} While traditionally unsupervised, IForest can be effectively used in a 
    semi-supervised manner by training its trees *only* on the "healthy" baseline data. 
    Anomalous points, which were not seen during training, will require fewer random "cuts" to be isolated, 
    resulting in a high anomaly score.

    \item \textbf{Autoencoders (AE):} As discussed in Section \ref{sec:prediction_methods}, this is a quintessential 
    semi-supervised model. The AE is trained to reconstruct *only* the normal baseline data. 
    When it is fed an anomalous data point, it will fail to reconstruct it accurately, leading to a high reconstruction error 
    (the anomaly score).

    \item \textbf{Recurrent Neural Networks (RNN/LSTM):} When used as a forecaster, an RNN/LSTM model is trained on sequences of 
    normal data to predict the next time step. Its anomaly score is the prediction error. If an anomaly occurs, 
    the model's prediction (based on normal patterns) will differ significantly from the anomalous reality.
    
    \item \textbf{Mixture Density Models (MDM):} These models, often combined with RNNs, do not just predict a single value. 
    Instead, they learn a full *probability distribution* of "normal" data (a "mixture" of multiple distributions). 
    The anomaly score is the negative log-likelihood; if an observed data point is highly improbable under the learned normal 
    distribution, it receives a high anomaly score.
    
    \item \textbf{Foundation Models (FM):} Large-scale models (e.g., \cite{TODO_Moirai2024, TODO_Chronos2024}) pre-trained on 
    diverse public datasets can be fine-tuned on our *specific* baseline energy data. This "few-shot learning" adapts the 
    model's general understanding of time series patterns to our building's specific "normal" behavior. 
    Anomalies are then detected as deviations from this fine-tuned baseline.
\end{itemize}
\chapter{Foundations}
\label{chap:fundamentals}

This chapter establishes the formal foundations required for contextual anomaly detection in building-energy telemetry. It characterizes the structural, statistical, and causal properties of building-energy data, defines the relevant anomaly taxonomies, and introduces the methodological and benchmarking concepts used throughout this thesis.

Based on these foundations, the chapter derives formal modeling requirements that constrain the design of detection, quantification, and attribution methodologies developed in the subsequent chapters.

\section{Characteristics of Building Energy Data}
\label{sec:building-energy-data}

Building-energy telemetry constitutes a multivariate, multimodal, and non-stationary stochastic process governed by physical, behavioural, and technical drivers. Effective anomaly detection therefore requires formal consideration of the structural and statistical properties of these data.

\subsection{Multivariate Structure}
\label{subsec:building-energy-multivariate-ts}

Building energy data is inherently multivariate and interdependent. In addition to aggregate meter readings, relevant variables include environmental conditions, occupancy, and subsystem states. Cross-variable dependencies are fundamental: changes in environmental drivers induce correlated changes in technical system loads. Consequently, anomaly detection must operate on multivariate joint behaviour rather than on isolated univariate series.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{images/multivariate-timeseries.png}
	\caption{Representative multivariate time series showing the main meter load together with occupancy (people count) and outdoor temperature. The plot illustrates how multiple interdependent variables evolve jointly over time.}
	\label{fig:multivariate-timeseries}
\end{figure}

\subsection{Causal Chain of Energy Consumption}
\label{subsec:causal-chain-energy-consumption}

Energy consumption emerges from a causal chain spanning demand generation, control logic, and mechanical execution. Environmental and occupancy conditions generate service demand; controllers translate demand into actuation commands; mechanical subsystems execute these commands, producing measurable energy use. Deviations observed at aggregate meters therefore frequently originate from faults located in upstream sensing or control layers.


\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{images/Energy-Causal-Chain.jpg}
	\caption{Causal chain of building energy consumption from demand over control to supply layer.}
	\label{fig:energy-causal-chain}
\end{figure}

Understanding the causal chain, as illustrated in Figure~\ref{fig:energy-causal-chain}, is a prerequisite for localizing anomalous behavior within building systems. A deviation observed in the building’s main meter often originates from a fault located in a preceding stage of the technical hierarchy, such as a sensor error or a logic failure in the control layer.

For instance, a malfunctioning temperature sensor reporting an erroneous heat spike triggers a cascade of responses. The control layer interprets this false data as a thermal requirement and initiates a cooling command to counteract the perceived heat. This signal causes the supply layer to activate mechanical components, such as cooling pumps and compressors. These devices consume electrical energy to satisfy the requested cooling load. Consequently, the aggregate output layer, represented by the building's main energy meter, records a significant increase in consumption. In this scenario, the measured energy spike is not a result of an actual physical need but acts as a symptom of a failure located deeper in the technical hierarchy.

\subsubsection{HVAC and Environmental Drivers}

HVAC systems dominate building energy demand. Thermal gradients, solar radiation, humidity, and scheduling logic determine cooling and heating loads. Suboptimal control strategies, scheduling conflicts, and mechanical degradation induce baseline drift and excessive consumption, generating anomalies that are often operationally normal yet energetically inefficient.


\subsubsection{Occupancy and Internal Loads}
\label{subsec:internal-loads-occupancy}

Human activity introduces stochastic variability through lighting, appliance use, and thermal gains. Behavioural interventions can decouple consumption from environmental drivers, while IT infrastructure introduces discrete operational regimes. These effects contribute to multimodality and regime-dependent energy patterns.

\subsubsection{Structural Moderators and Data Integrity}
\label{subsec:structural-technical-influences}

Building envelope characteristics and thermal inertia modulate system response dynamics. Interdependencies between subsystems propagate anomalies across services. Digital measurement infrastructure introduces non-physical artifacts, including missing values and aggregation spikes, which must be distinguished from physical faults during preprocessing.


\subsection{Temporal Dependence and Persistence}
\label{subsec:temporal-autocorrelation-persistence}

Building-energy telemetry exhibits strong temporal autocorrelation caused by thermal inertia, operational ramp-up dynamics, and persistent high-load device states. Consequently, short-term system behaviour is highly predictable under nominal operation, while slow-developing faults and sustained inefficiencies may remain concealed within otherwise smooth trajectories.

This persistence simultaneously stabilizes short-term forecasting and undermines detection of long-duration anomalies, particularly when sequential models rapidly absorb anomalous regimes into their predictive baseline.

\subsection{Seasonality and Periodicity}
\label{subsec:periodic-variations-seasonality}

Building-energy consumption follows pronounced daily, weekly, and seasonal periodicities driven by occupancy cycles, control schedules, and climatic seasons. These regime-dependent patterns form a repetitive operational fingerprint.

Anomaly detection must therefore distinguish contextual violations of expected periodic regimes (e.g., weekday-level consumption during weekends) from absolute deviations.
\subsection{Statistical Distribution and Non-Stationarity}
\label{subsec:statistical-distribution-stochastic-noise}

Empirical building-energy distributions deviate substantially from unimodal Gaussian assumptions and exhibit multimodal mixture structures with heavy tails due to discrete operational regimes and heterogeneous subsystem interactions. Deterministic point estimates are therefore insufficient to represent normative behaviour.

Sparse coverage of extreme weather and rare operational states introduces causal ambiguity and increases false anomaly rates for previously unobserved but physically valid conditions. Furthermore, building-energy telemetry is non-stationary; long-term baseline drift caused by seasonal transitions, equipment degradation, and persistent occupancy changes continuously shifts normative distributions, necessitating probabilistic, context-aware modeling.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{images/mixture-destribution.png}
	\caption{Empirical distribution of the building's main meter (15-minute kWh values, histogram) with an overlaid single normal distribution and a Gaussian mixture model with five components, illustrating the mismatch between a unimodal Gaussian model and the multimodal, heavy-tailed structure of real building energy data.}
	\label{fig:mixture-distribution}
\end{figure}

Figure~\ref{fig:mixture-distribution} illustrates this effect on real data: the measured main-meter consumption concentrates in several dense regions at lower loads and exhibits a pronounced right tail. A single normal distribution smooths over these structures and underestimates tail probabilities, whereas the fitted Gaussian mixture adapts to the multiple modes and better traces the empirical density.


\subsection{Data Acquisition and Semantic Structure}
\label{subsec:data-acquisition-telemetry}

The transformation of physical energy consumption into digital telemetry follows a multi-stage acquisition pipeline that converts electrical quantities into structured multivariate time series suitable for algorithmic analysis.

\begin{description}
	\item[Ontological Modeling:] To ensure interpretability and causal localization, the telemetry is mapped to a semantic ontology that encodes the physical and logical relationships between meters, subsystems, and devices~\cite{ElionaOntologies}. This ontological layer enables detected anomalies to be localized within the technical hierarchy rather than remaining aggregated deviations at the main meter level.
	\item[Standardized Units:] Raw meter readings are converted into standardized physical units (e.g., kWh) to ensure consistency across heterogeneous hardware and communication interfaces.
\end{description}

\subsection{Data Continuity and Transmission Artifacts}
\label{subsec:communication-artifacts}

The integrity of telemetry streams depends on the stability of the communication infrastructure. Network-level distortions introduce non-physical artifacts that must be distinguished from actual building faults.

\begin{description}
	\item[Transmission Gaps:] Communication failures produce missing values that interrupt temporal continuity and require correction during preprocessing.
	\item[Aggregation Spikes:] Buffered data retransmission following outages may produce virtual load spikes, reflecting delayed reporting rather than physical surges in energy demand.
\end{description}

\section{Foundations of Anomaly Detection}
\label{sec:fundamentals-anomaly-detection}

Anomaly detection aims to identify observations or patterns that deviate from an implicit notion of normality. In time-series anomaly detection (TSAD), deviations are defined relative to temporal structure, persistence, and regime-dependent behaviour rather than isolated numerical values. The taxonomy adopted in this work follows the benchmark framework proposed by Paparrizos et al.~\cite{paparrizos2022tsb}.

\subsection{Dimensionality and Normality Regimes}
\label{subsec:dimensionality-normality-modes}

The complexity of anomaly detection is governed by the dimensionality of the time series and the number of normative operational regimes.

\begin{description}
	\item[Dimensionality:] Univariate time series describe a single system variable, whereas multivariate time series jointly model multiple interdependent variables. The dataset analyzed in this work is multivariate, combining aggregate energy consumption with environmental and occupancy drivers to resolve causal ambiguity.

	\item[Normality Regimes:] Building-energy telemetry operates under multiple normative regimes driven by seasonal, operational, and occupancy-dependent contexts. Consequently, “normal” behaviour is regime-specific rather than globally invariant.
\end{description}

The analyzed data is therefore classified as multivariate with multi-mode normality, necessitating detection models that adapt to shifting baselines and cross-variable dependencies.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{images/Univariate-Multivariate-normalities.jpg}
	\caption{Schematic illustration of time series types along two axes---dimensionality (univariate vs.~multivariate) and normality regimes (single-mode vs.~multi-mode). Adapted from Boniol et al.'s tutorial on new trends in time series anomaly detection~\cite{Boniol2023NewTrends}.}
	\label{fig:univariate-multivariate-normalities}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{images/types-anomalies.jpg}
	\caption{Taxonomy of time series anomalies along structural and multiplicity dimensions, distinguishing global and contextual point anomalies, subsequence-based anomalies, and their occurrence as single, multiple different, or multiple similar events. Adapted from Boniol et al.'s tutorial on new trends in time series anomaly detection~\cite{Boniol2023NewTrends}.}
	\label{fig:anomaly-taxonomy}
\end{figure}

\subsection{Structural Classes of Anomalies}

\begin{description}
  \item[Point Anomalies:] Individual observations that deviate from expected behaviour.
  \item[Global Point Anomalies:] Deviations outside the global historical range.
  \item[Contextual Point Anomalies:] Deviations from regime-dependent normative behaviour defined by temporal or exogenous context.
  \item[Subsequence Anomalies:] Deviations manifested through abnormal temporal patterns.
\end{description}

\subsection{Multiplicity of Occurrence}

\begin{description}
  \item[Single Anomalies:] Isolated anomalous events.
  \item[Multiple Similar Anomalies:] Recurrent manifestations of the same anomaly pattern.
  \item[Multiple Different Anomalies:] Co-occurring anomalies of heterogeneous types.
\end{description}


\section{Methodological Approaches to Anomaly Detection}
\label{sec:methodological-approaches}

Anomaly detection transforms raw time-series telemetry into actionable information by assigning each observation a degree of abnormality and mapping it to a binary decision boundary.

\subsection{Anomaly Scores}

Most detection algorithms output an anomaly score $s_i$ per timestamp, which quantifies deviation from learned normative behaviour. Binary alerts are obtained by thresholding this score, yielding a time series of nominal and anomalous states.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{images/anomaly_score.png}
	\caption{Example of an anomaly score $s_i \in [0,1]$ aligned with the underlying time series. A threshold of 0.8 separates normal points from those flagged as anomalous.}
	\label{fig:anomaly-score}
\end{figure}

\subsection{Learning Paradigms}

The applicability of detection methods is governed by the availability of labelled data:
\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{images/superviced-unsuperviced.jpg}
	\caption{Schematic overview of supervised, semi-supervised, and unsupervised learning paradigms for anomaly detection, adapted from Boniol et al.'s tutorial on new trends in time series anomaly detection~\cite{Boniol2023NewTrends}.}
	\label{fig:learning-paradigms}
\end{figure}

\begin{description}
	\item[Supervised:] Requires explicit labels for both normal and anomalous states; rarely feasible in building operations.
	\item[Semi-Supervised:] Learns normative behaviour from assumed healthy historical data; commonly used in building-energy monitoring.
	\item[Unsupervised:] Operates without labelled baselines; typically applied during system commissioning or cold-start phases.
\end{description}



\subsection{Families of Detection Methods}

Anomaly detection approaches are grouped into three methodological families:

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{images/distance-dencity-forecast.jpg}
	\caption{Hierarchical taxonomy of anomaly detection methods grouped into distance-based, density-based, and prediction-based families, adapted from Boniol et al.'s tutorial on new trends in time series anomaly detection~\cite{Boniol2023NewTrends}.}
	\label{fig:method-taxonomy}
\end{figure}

\begin{description}
	\item[Distance-Based:] Identify anomalous subsequences by pattern dissimilarity.
	\item[Density-Based:] Detect low-probability observations in learned feature distributions.
	\item[Prediction-Based:] Identify deviations via residuals between predicted and observed values, including forecasting- and reconstruction-based variants.
\end{description}

This thesis focuses on prediction-based approaches, as they are the only methodological family that provides an explicit expected-value baseline, enabling deviations to be quantified in physical units and directly translated into financial impact. This property is essential for contextual building-energy anomaly detection and economic loss estimation.
\section{Benchmarking Foundations}
\label{sec:benchmarking-foundations}

Benchmarking evaluates anomaly detection performance against labelled reference datasets by comparing model decisions to ground-truth annotations.

\subsection{Binary Labels and Confusion Matrix}

Each observation is assigned a binary label (normal vs. anomalous). Model predictions are evaluated using the confusion matrix, yielding counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).

\subsection{Evaluation Metrics}

Performance is summarized using precision, recall, and the F1 score:

\[
\mathrm{Precision} = \frac{TP}{TP+FP}, \quad
\mathrm{Recall} = \frac{TP}{TP+FN}, \quad
F1 = 2 \cdot \frac{\mathrm{Precision}\cdot\mathrm{Recall}}{\mathrm{Precision}+\mathrm{Recall}}.
\]

These metrics quantify alarm reliability, detection sensitivity, and their balanced trade-off, respectively.

\section{Synthesis of Foundations}
\label{sec:synthesis-foundations}

Building-energy telemetry constitutes a multivariate, multimodal, and non-stationary stochastic process governed by a causal chain spanning demand, control, and mechanical execution. Deviations observed at aggregate meters therefore represent manifestations of upstream technical or behavioural faults rather than isolated numerical outliers.

From the structural and statistical properties of these data, the following formal requirements for anomaly detection follow:

\begin{enumerate}
	\item \textbf{Probabilistic multivariate modeling}\\
	Expected behaviour must be represented as a multivariate mixture distribution rather than a deterministic point estimate, in order to capture multimodal operating regimes and cross-variable dependencies.

	\item \textbf{Robustness to temporal dependence and persistence}\\
	Detection must remain stable under strong autocorrelation, seasonal regime shifts, and long-duration persistence, such that sustained deviations are not absorbed into the learned baseline.

	\item \textbf{Separation of physical faults and digital artifacts}\\
	Transmission gaps, aggregation spikes, and other digital artifacts must be distinguished from physical anomalies through explicit preprocessing and data-quality handling.
\end{enumerate}

Furthermore, building-energy telemetry is formally classified as a multivariate, multi-mode time series dominated by contextual point anomalies. Subsequence faults therefore manifest as contextual point deviations at the aggregation horizons relevant for energy monitoring.  
Persistent deviations present in historical baselines may be absorbed into learned normality (normality drift), representing a fundamental constraint for semi-supervised and unsupervised detection paradigms.
